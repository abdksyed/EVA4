{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch101.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abksyed/EVA4/blob/master/03PyToch101/PyTorch101_comp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kYM3ylzYo1Q",
        "colab_type": "text"
      },
      "source": [
        "![PyTorch](https://devblogs.nvidia.com/wp-content/uploads/2017/04/pytorch-logo-dark.png)\n",
        "\n",
        "An open source machine learning framework that accelerates the path from research prototyping to production deployment.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WcVqSTrWY6oz",
        "colab_type": "text"
      },
      "source": [
        "# Tensor - Pytorch's core data structure\n",
        "\n",
        "In Python we can create lists, lists of lists, lists of lists and so on. In NumPy there is a `numpy.ndarray` which represents `n`- dimensional array. In math there is a special name for the generalization of vectors and matrices to a higher dimensional space - a tensor\n",
        "\n",
        "Tensor is an entity with a defined number of dimensions called an order (rank). \n",
        "\n",
        "**Scalar** can be considered as a rank-0-tensor. \n",
        "\n",
        "**Vector** can be introduced as a rank-1-tensor. \n",
        "\n",
        "**Matrices** can be considered as a rank-2-tensor.\n",
        "\n",
        "# Tensor Basics\n",
        "\n",
        "Let's import the torch module first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-oFVL2tYYqp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGxuR5IEaFJa",
        "colab_type": "text"
      },
      "source": [
        "## Tensor Creation\n",
        "Let's view examples of matrices and tensors generation\n",
        "\n",
        "2-dimensional (rank-2) tensor of zeros:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "897JQc25aD3E",
        "colab_type": "code",
        "outputId": "21d0b46d-f59a-471c-a7dd-073873418468",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "torch.zeros(3, 4)"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pujmpEBhaPRA",
        "colab_type": "text"
      },
      "source": [
        "Random rank-3 tensor:\n",
        "_read the print below and convince yourself how this is a rank-3-tensor and learn what those 2, 3, 4 values are there for_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBrznTNhaOL7",
        "colab_type": "code",
        "outputId": "ab5e5af6-69e6-42b3-df37-dd169348842d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "torch.rand(2, 3, 4)"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.2820, 0.6914, 0.4831, 0.2919],\n",
              "         [0.8257, 0.6144, 0.5993, 0.5940],\n",
              "         [0.9721, 0.0513, 0.7880, 0.2941]],\n",
              "\n",
              "        [[0.0732, 0.6776, 0.8928, 0.1070],\n",
              "         [0.7695, 0.7220, 0.4020, 0.0615],\n",
              "         [0.7726, 0.1053, 0.8774, 0.1177]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kO4fIr15asdi",
        "colab_type": "text"
      },
      "source": [
        "I am hoping you have noticed 4-elements in a row, 3 rows making one block and there are 2 blocks. \n",
        "\n",
        "Random rank-4-tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiCLvMGCaVZ3",
        "colab_type": "code",
        "outputId": "f96c1684-f662-45c9-be69-7b1dc8725d65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "torch.rand(2, 2, 2, 3)"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[0.0037, 0.1836, 0.4515],\n",
              "          [0.4034, 0.1959, 0.6056]],\n",
              "\n",
              "         [[0.1013, 0.6996, 0.4221],\n",
              "          [0.4231, 0.1687, 0.2169]]],\n",
              "\n",
              "\n",
              "        [[[0.9579, 0.3588, 0.0993],\n",
              "          [0.8030, 0.3363, 0.8410]],\n",
              "\n",
              "         [[0.8107, 0.1109, 0.4863],\n",
              "          [0.1030, 0.1754, 0.9215]]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF6VL68s3AMV",
        "colab_type": "text"
      },
      "source": [
        "## Question 1:\n",
        "\n",
        "How many dimensions are there in a tensor defined as below?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IbXcI3x3Ibt",
        "colab_type": "code",
        "outputId": "a1a0e5cd-fa4b-46a3-8908-9e8047372e34",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "torch.rand(1, 1, 1, 1)"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[0.2188]]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkvKAjpqcaUO",
        "colab_type": "text"
      },
      "source": [
        "**4**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmgpqssRa9jF",
        "colab_type": "text"
      },
      "source": [
        ".\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "There are many more ways to create tensor using some restrictions on values it should contatn - for the full reference, please follow the [official docs](https://pytorch.org/docs/stable/torch.html#creation-ops). \n",
        "\n",
        "\n",
        ".\n",
        "---\n",
        "\n",
        "\n",
        "# Python / NumPy / Pytorch interoperability\n",
        "\n",
        "You can create tensors from python as well as numpy arrays. You can also convert torch tensors to numpy arrays. So, the interoperability between torch and numpy is pretty good. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ipgpeWPfa6gE",
        "colab_type": "code",
        "outputId": "8517ce70-1739-4f2f-db4d-9e57516bf390",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# Simple Python List\n",
        "python_list = [1, 2]\n",
        "\n",
        "# Create a numpy array from python list\n",
        "numpy_array = np.array(python_list)\n",
        "\n",
        "# Create a torch Tensor from python list\n",
        "tensor_from_list = torch.tensor(python_list)\n",
        "\n",
        "# Create a torch Tensor from Numpy array\n",
        "tensor_from_array = torch.tensor(numpy_array)\n",
        "\n",
        "# Another way to create a torch Tensor from Numpy array (share same storage)\n",
        "tensor_from_array_v2 = torch.from_numpy(numpy_array)\n",
        "\n",
        "# Convert torch tensor to numpy array\n",
        "array_from_tensor = tensor_from_array.numpy()\n",
        "\n",
        "print('List:   ', python_list)\n",
        "print('Array:  ', numpy_array)\n",
        "print('Tensor: ', tensor_from_list)\n",
        "print('Tensor: ', tensor_from_array)\n",
        "print('Tensor: ', tensor_from_array_v2)\n",
        "print('Array:  ', array_from_tensor)"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "List:    [1, 2]\n",
            "Array:   [1 2]\n",
            "Tensor:  tensor([1, 2])\n",
            "Tensor:  tensor([1, 2])\n",
            "Tensor:  tensor([1, 2])\n",
            "Array:   [1 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_x-86B8gqik",
        "colab_type": "text"
      },
      "source": [
        "**Difference between** `torch.Tensor` **and** `torch.from_numpy`\n",
        "\n",
        "Pytorch aims to be an effective library for computations. What does it mean? It means that pytorch avoids memory copying if it can. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHCKDGpygn_d",
        "colab_type": "code",
        "outputId": "6ee89168-a9f8-43e7-ed05-a2d2b8296352",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "numpy_array[0] = 10\n",
        "\n",
        "print('Array:  ', numpy_array)\n",
        "print('Tensor: ', tensor_from_array)\n",
        "print('Tensor: ', tensor_from_array_v2)"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Array:   [10  2]\n",
            "Tensor:  tensor([1, 2])\n",
            "Tensor:  tensor([10,  2])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lm2rvf7sR32Y",
        "colab_type": "text"
      },
      "source": [
        "**Accuracy Increases**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CKtdNRM3SMp",
        "colab_type": "text"
      },
      "source": [
        "## Question 2:\n",
        "\n",
        "Assume that we moved our complete (cats vs dogs) image dataset to numpy arrays. Then we use torch.from_numpy to convert these images to tensor. Then we apply a specific data augmentation strategy called \"CutOut\" which blocks a portion of the image directly on these tensors. What will happen to the accuracy of a model trained on this strategy compared to the one without this strategy? CutOut strategy is shown below: \n",
        "\n",
        "![CutOut](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcSnSyN835AmtQPKQbPjDHX-FmshNilbtexX95cRGQPwl56QCGDn)\n",
        "\n",
        "## Question 3:\n",
        "Why do you think we are observing this behavior?\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwJ8ROw9eu7Z",
        "colab_type": "text"
      },
      "source": [
        "**When using cutout, we are asking the model to learn even the minute features from the images too, by hiding some major features, which may lead to better accuracy, so the Model Accuracy Increases after CutOut**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZq7O-aihSOA",
        "colab_type": "text"
      },
      "source": [
        "We have two different ways to create tensor from its NumPy counterpart - one copies memory and another one shares the same underlying storage. It works in the opposite way:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNFOwV8EhPwQ",
        "colab_type": "code",
        "outputId": "8ece5009-ab9b-4440-c0ab-9cf3ce7849bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "array_from_tensor = tensor_from_array.numpy()\n",
        "array_from_tensor_v2 = tensor_from_array_v2.numpy()\n",
        "print('Tensor: ', tensor_from_array)\n",
        "print('Array: ', array_from_tensor)\n",
        "print('Tensor: ', tensor_from_array_v2)\n",
        "print('Array v2: ', array_from_tensor_v2)\n",
        "print()\n",
        "tensor_from_array[0] = 11\n",
        "tensor_from_array_v2[0] = 11\n",
        "\n",
        "print('Tensor: ', tensor_from_array)\n",
        "print('Array: ', array_from_tensor)\n",
        "print('Tensor: ', tensor_from_array_v2)\n",
        "print('Array v2: ', array_from_tensor_v2)"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor:  tensor([1, 2])\n",
            "Array:  [1 2]\n",
            "Tensor:  tensor([10,  2])\n",
            "Array v2:  [10  2]\n",
            "\n",
            "Tensor:  tensor([11,  2])\n",
            "Array:  [11  2]\n",
            "Tensor:  tensor([11,  2])\n",
            "Array v2:  [11  2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UM9ytbvKhtCw",
        "colab_type": "text"
      },
      "source": [
        "## Data types\n",
        "\n",
        "The basic data type of all Deep Learning-related operations is float, but sometimes you may need something else. Pytorch support different number types for its tensors the same way NumPy does it - by specifying the data type on tensor creation or via casting. Ths full list of supported data types can be found [here](https://pytorch.org/docs/stable/tensors.html). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bd6WkzJ4hpYi",
        "colab_type": "code",
        "outputId": "ee1a358a-7732-48ea-944d-92c5108cf4f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "tensor = torch.zeros(2, 2)\n",
        "print('Tensor with default type: ', tensor)\n",
        "tensor = torch.zeros(2, 2, dtype=torch.float16)\n",
        "print('Tensor with 16-bit float: ', tensor)\n",
        "tensor = torch.zeros(2, 2, dtype=torch.int16)\n",
        "print('Tensor with integers: ', tensor)\n",
        "tensor = torch.zeros(2, 2, dtype=torch.bool)\n",
        "print('Tensor with boolean data: ', tensor)"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor with default type:  tensor([[0., 0.],\n",
            "        [0., 0.]])\n",
            "Tensor with 16-bit float:  tensor([[0., 0.],\n",
            "        [0., 0.]], dtype=torch.float16)\n",
            "Tensor with integers:  tensor([[0, 0],\n",
            "        [0, 0]], dtype=torch.int16)\n",
            "Tensor with boolean data:  tensor([[False, False],\n",
            "        [False, False]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9F4Dkdr40TE",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Question 4:\n",
        "We saw above that some times numpy and tensors share same storage and changing one changes the other. \n",
        "If we define a rank-2-tensor with ones (dtype of f16), and then convert it into a numpy data type using tensor.numpy() and store it in a variable called \"num\", and then we perform this operation `num = num * 0.5`, will the original tensor have 1.0s or 0.5s as its element values? \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ksDW3ezgdDT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "66e7970e-8c2e-4f2c-d62d-0ad1f03062b8"
      },
      "source": [
        "ones = torch.ones(3,4, dtype=torch.float16)\n",
        "print(ones)\n",
        "num = ones.numpy()\n",
        "print(num)\n",
        "num = num * 0.5\n",
        "print(ones)\n",
        "print(num)"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.]], dtype=torch.float16)\n",
            "[[1. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]]\n",
            "tensor([[1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.]], dtype=torch.float16)\n",
            "[[0.5 0.5 0.5 0.5]\n",
            " [0.5 0.5 0.5 0.5]\n",
            " [0.5 0.5 0.5 0.5]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4IVmQT_gVqI",
        "colab_type": "text"
      },
      "source": [
        "**The Label 'num' has been moved from all 1.0s to all 0.5s, but the label 'ones' will remain at 1.0s**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiQt8Mmm51OE",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Question 5: \n",
        "If the operation `num = num*5` is changed to `num[:] = num*5` will the original tensor have 1.0s or 0.5s as its element values? \n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fs2kAwaPhkGf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "bd4659a8-f844-4908-cf79-e6053122329a"
      },
      "source": [
        "ones = torch.ones(3,4, dtype= torch.float16)\n",
        "print(ones)\n",
        "num = ones.numpy()\n",
        "print(num)\n",
        "num[:] = num * 0.5\n",
        "print(ones)\n",
        "print(num)"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.],\n",
            "        [1., 1., 1., 1.]], dtype=torch.float16)\n",
            "[[1. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]\n",
            " [1. 1. 1. 1.]]\n",
            "tensor([[0.5000, 0.5000, 0.5000, 0.5000],\n",
            "        [0.5000, 0.5000, 0.5000, 0.5000],\n",
            "        [0.5000, 0.5000, 0.5000, 0.5000]], dtype=torch.float16)\n",
            "[[0.5 0.5 0.5 0.5]\n",
            " [0.5 0.5 0.5 0.5]\n",
            " [0.5 0.5 0.5 0.5]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slPRfeaihrHQ",
        "colab_type": "text"
      },
      "source": [
        "**Now we are changing the values in the label num, so this changes will automatically show up in ones(tensor)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzh8UB8KiVmb",
        "colab_type": "text"
      },
      "source": [
        "## Indexing\n",
        "\n",
        "Tensor provides access to its elements via the same `[]` operation as a regular python list or NumPy array. However, as you may recall from NumPy usage, the full power of math libraries is accessible only via vectorized operations, i.e. operations without explicit looping over all vector elements in python and using implicit optimized loops in C/C++/CUDA/Fortran/etc. available via special function calls. Pytorch employs the same paradigm and provides a wide range of vectorized operations. Let's take a look at some examples. \n",
        "\n",
        "Joining a list of tensors together with `torch.cat`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMaCDKPhiUAb",
        "colab_type": "code",
        "outputId": "dd97ac33-e5c4-46ab-e668-17f56f40fcad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "a = torch.zeros(3, 2)\n",
        "b = torch.ones(3, 2)\n",
        "print(torch.cat((a, b), dim=0))"
      ],
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        [1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bj4aeE86zdH",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Question 6: \n",
        "Is the transpose of concatenated a & b tensor on dimension 1, same as the contatenated tensor of a & b on dimension 0? \n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VIbkpw4TqNY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "2c7ff7f3-6b77-40b5-d3bd-834e852f3572"
      },
      "source": [
        "a = torch.zeros(3, 2)\n",
        "b = torch.ones(3, 2)\n",
        "concat = torch.cat((a, b), dim=0)\n",
        "concat1 = torch.cat((a, b), dim=1)\n",
        "concatT = concat1.T\n",
        "\n",
        "print(concat)\n",
        "print(concat1)\n",
        "print(concatT)"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0., 0.],\n",
            "        [0., 0.],\n",
            "        [0., 0.],\n",
            "        [1., 1.],\n",
            "        [1., 1.],\n",
            "        [1., 1.]])\n",
            "tensor([[0., 0., 1., 1.],\n",
            "        [0., 0., 1., 1.],\n",
            "        [0., 0., 1., 1.]])\n",
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.],\n",
            "        [1., 1., 1.],\n",
            "        [1., 1., 1.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjF22TXKUaQD",
        "colab_type": "text"
      },
      "source": [
        "**NO Transpose of concat of a&b on dim=1 is NOT same as concat of ab on dim=0**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TXNne69j7LP",
        "colab_type": "text"
      },
      "source": [
        "Indexing with another tenxor/array:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiE-Fsi4jVd6",
        "colab_type": "code",
        "outputId": "6dfd521f-0bea-4f05-90b1-aa3f2ffb2dfa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "a = torch.arange(start=0, end=10)\n",
        "indices = np.arange(0, 10) > 5\n",
        "print(a)\n",
        "print(indices)\n",
        "print(a[indices])\n",
        "\n",
        "indices = torch.arange(start=0, end=10) %5\n",
        "print(indices)\n",
        "print(a[indices])"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
            "[False False False False False False  True  True  True  True]\n",
            "tensor([6, 7, 8, 9])\n",
            "tensor([0, 1, 2, 3, 4, 0, 1, 2, 3, 4])\n",
            "tensor([0, 1, 2, 3, 4, 0, 1, 2, 3, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LS4dnlu47WQu",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Question 7:\n",
        "\n",
        "`a` is defined as `torch.arange(start=0, end=10)`. We will create `b` using the two operations as below. In both cases do we get the same value?\n",
        "\n",
        "\n",
        "1.   indices variable created by the modulo operation on arange between 0 and 10. Then a new varialble `b` is created from `a` using the last 5 elements of indices. \n",
        "2.   indices variable created by the modulo operation on arange betwenn 1 and 11. Then a new varialble `b` is created from `a` using the last 5 elements of indices. \n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i0fWtFGxVLAg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "63e70b55-2ecf-4417-abee-361bebfa8c87"
      },
      "source": [
        "a = torch.arange(start=0, end=10)\n",
        "\n",
        "indicesa = torch.arange(start=0, end=10) %5\n",
        "print(indicesa)\n",
        "print(a[indicesa])\n",
        "\n",
        "indicesb = torch.arange(start=1, end=11) %5\n",
        "print(indicesb)\n",
        "print(a[indicesb])"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0, 1, 2, 3, 4, 0, 1, 2, 3, 4])\n",
            "tensor([0, 1, 2, 3, 4, 0, 1, 2, 3, 4])\n",
            "tensor([1, 2, 3, 4, 0, 1, 2, 3, 4, 0])\n",
            "tensor([1, 2, 3, 4, 0, 1, 2, 3, 4, 0])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLYPIv_rVf2t",
        "colab_type": "text"
      },
      "source": [
        "**NO, we get different b**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQ4ZCVsTk-KH",
        "colab_type": "text"
      },
      "source": [
        "What should we do if we have, say, rank-2-tensor and want to select only some rows?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GtRpotjkt1q",
        "colab_type": "code",
        "outputId": "79de4819-7bd5-4b62-a445-84896538f79f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "tensor = torch.rand((5, 3))\n",
        "rows = torch.tensor([0, 4])\n",
        "print(tensor)\n",
        "print(rows)\n",
        "tensor[rows]"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.5964, 0.2671, 0.5366],\n",
            "        [0.0576, 0.4843, 0.1726],\n",
            "        [0.5012, 0.2892, 0.5416],\n",
            "        [0.3103, 0.0407, 0.6339],\n",
            "        [0.7747, 0.8756, 0.4561]])\n",
            "tensor([0, 4])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.5964, 0.2671, 0.5366],\n",
              "        [0.7747, 0.8756, 0.4561]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIJnr_N2_Qaf",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Question 8: \n",
        "\n",
        "Consider a tensor defined as `torch.rand((6, 5))`. Is the shape of the new tensor created by taking the 0th, 2nd and 4th row of the old tensor same as the shape of the a newer tensor created by taking the 0th, 2nd and 4th row of the old tensor after transposing it by operation `torch.transpose(tensor, 0, 1)` ?\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hm2cqSbMlO8u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1a5efa02-5632-44d5-c4dc-9f8b74cb5855"
      },
      "source": [
        "a = torch.rand((6,5))\n",
        "print(a[[0,2,4]].shape)\n",
        "b = a.T\n",
        "print(b[[0,2,4]].shape)"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 5])\n",
            "torch.Size([3, 6])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Usww32xhl0QF",
        "colab_type": "text"
      },
      "source": [
        "**NO, the Shapes are different, it's 3x5 an 3x6**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "naDzFMkslU0b",
        "colab_type": "text"
      },
      "source": [
        "## Tensor Shapes\n",
        "\n",
        "Reshaping a tensor is a frequently used operation. We can change the shape of a tensor without the memory copying overhead. There are two methods for that: `reshape` and `view`. \n",
        "\n",
        "The difference is the following: \n",
        "\n",
        "\n",
        "*   view tries to return a tensor, and it shares the same memory with the original tensor. In case, if it cannot reuse the same memory due to [some reason](https://pytorch.org/docs/stable/tensors.html?highlight=view#torch.Tensor.view), it just fails. \n",
        "*   reshape always returns the tensor with the desired shape and tries to reuse the memory. If it cannot, it creates a copy\n",
        "\n",
        "Let's see with the help of an example:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HClDkqLLlMJh",
        "colab_type": "code",
        "outputId": "a7e79490-d51d-47cc-93f7-f4ff4c1fbd87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        }
      },
      "source": [
        "tensor = torch.rand(2, 3, 4)\n",
        "print('Pointer to data: ', tensor.data_ptr())\n",
        "print('Shape: ', tensor.shape)\n",
        "\n",
        "reshaped = tensor.reshape(24)\n",
        "\n",
        "view = tensor.view(3, 2, 4)\n",
        "print('Reshaped tensor - pointer to data', reshaped.data_ptr())\n",
        "print('Reshaped tensor shape ', reshaped.shape)\n",
        "\n",
        "print('Viewed tensor - pointer to data', view.data_ptr())\n",
        "print('Viewed tensor shape ', view.shape)\n",
        "\n",
        "print()\n",
        "print(tensor)\n",
        "print(reshaped)\n",
        "print(view)\n",
        "print()\n",
        "\n",
        "assert tensor.data_ptr() == view.data_ptr()\n",
        "\n",
        "assert np.all(np.equal(tensor.numpy().flat, reshaped.numpy().flat))\n",
        "\n",
        "print('Original stride: ', tensor.stride())\n",
        "print('Reshaped stride: ', reshaped.stride())\n",
        "print('Viewed stride: ', view.stride())"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pointer to data:  102595584\n",
            "Shape:  torch.Size([2, 3, 4])\n",
            "Reshaped tensor - pointer to data 102595584\n",
            "Reshaped tensor shape  torch.Size([24])\n",
            "Viewed tensor - pointer to data 102595584\n",
            "Viewed tensor shape  torch.Size([3, 2, 4])\n",
            "\n",
            "tensor([[[0.9202, 0.1293, 0.7200, 0.9303],\n",
            "         [0.3450, 0.4786, 0.4762, 0.0730],\n",
            "         [0.5707, 0.0634, 0.7531, 0.8654]],\n",
            "\n",
            "        [[0.1642, 0.1648, 0.5201, 0.4134],\n",
            "         [0.6589, 0.0897, 0.7869, 0.1855],\n",
            "         [0.7867, 0.5314, 0.2357, 0.0796]]])\n",
            "tensor([0.9202, 0.1293, 0.7200, 0.9303, 0.3450, 0.4786, 0.4762, 0.0730, 0.5707,\n",
            "        0.0634, 0.7531, 0.8654, 0.1642, 0.1648, 0.5201, 0.4134, 0.6589, 0.0897,\n",
            "        0.7869, 0.1855, 0.7867, 0.5314, 0.2357, 0.0796])\n",
            "tensor([[[0.9202, 0.1293, 0.7200, 0.9303],\n",
            "         [0.3450, 0.4786, 0.4762, 0.0730]],\n",
            "\n",
            "        [[0.5707, 0.0634, 0.7531, 0.8654],\n",
            "         [0.1642, 0.1648, 0.5201, 0.4134]],\n",
            "\n",
            "        [[0.6589, 0.0897, 0.7869, 0.1855],\n",
            "         [0.7867, 0.5314, 0.2357, 0.0796]]])\n",
            "\n",
            "Original stride:  (12, 4, 1)\n",
            "Reshaped stride:  (1,)\n",
            "Viewed stride:  (8, 4, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIN5jSppm4yC",
        "colab_type": "text"
      },
      "source": [
        "The basic rule about reshaping the tensor is definitely that you cannot change the total number of elements in it, so the product of all tensor's dimensions should always be the same. It gives us the ability to avoid specifying one dimension when reshaping the tensor - Pytorch can calculate it for us:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3D19ERFmzOl",
        "colab_type": "code",
        "outputId": "92f0531f-bd91-4d90-f129-e17216dee340",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(tensor.reshape(3, 2, 4).shape)\n",
        "print(tensor.reshape(3, 2, -1).shape)\n",
        "print(tensor.reshape(3, -1, 4).shape)"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 2, 4])\n",
            "torch.Size([3, 2, 4])\n",
            "torch.Size([3, 2, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObgCQKUiETak",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## Question 9:\n",
        "\n",
        "Consider a tensor `a` created with [1, 2, 3] and [1, 2, 3] of size (2, 3) is reshaped with operation `.reshape(-1, 2)`. Also consider a tensor `b` created with [[2, 1]] and of size (1, 2), later operated with `view(2, -1)` operation. \n",
        "\n",
        "If we do a dot product of a and b (using `torch.mm`) and perform the sum of all the elements (using `torch.sum`) what do we get? (enter int value without any decimal point in the quiz)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNYe6WbjnSp-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "931db193-ee5e-4098-e61b-311f155e1e01"
      },
      "source": [
        "a = torch.tensor([[1,2,3], [1,2,3]])\n",
        "print(a)\n",
        "print(a.reshape(-1,2))\n",
        "\n",
        "print()\n",
        "\n",
        "b = torch.tensor([[2, 1]])\n",
        "print(b)\n",
        "print(b.view(2,-1))\n",
        "\n",
        "torch.sum(torch.mm(a.reshape(-1,2), b.view(2,-1)))"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1, 2, 3],\n",
            "        [1, 2, 3]])\n",
            "tensor([[1, 2],\n",
            "        [3, 1],\n",
            "        [2, 3]])\n",
            "\n",
            "tensor([[2, 1]])\n",
            "tensor([[2],\n",
            "        [1]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(18)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOlJv8NBoBiz",
        "colab_type": "text"
      },
      "source": [
        "**18**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mza7QPg3ndeV",
        "colab_type": "text"
      },
      "source": [
        "**Alternative ways to view tensors** - `expand` or `expand_as`.\n",
        "\n",
        "\n",
        "\n",
        "*   `expand` - requires the desired shape as an input\n",
        "*   `expand_as` - uses the shape of another tensor\n",
        "\n",
        "These operations \"repeat\" tensor's values along the specified axes without actually copying the data. \n",
        "\n",
        "As the documentation says, expand:\n",
        "\n",
        "\n",
        "> returns a new view of the self tensor with singleton dimensions expanded to a larger size. Tensor can be also expanded to a larger number of dimensions, and the new ones will be appended at the front. For the new dimensions, the size cannot be set to -1. \n",
        "\n",
        "**Use case:**\n",
        "\n",
        "\n",
        "\n",
        "*   index multi-channel tensor with single-channel mask - imagine a color image with 3 channels (RGB) and binary mask for the area of interest on that image. We cannot index the image with this kind of mask directly since the dimensions are different, but we can use `expand_as` operation to create a view of the mask that has the same dimensions as the image we want to apply it to, but has not copied the data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iz33E-V7nPQT",
        "colab_type": "code",
        "outputId": "08225aac-9f3a-4f65-e10f-02401e7a92b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        }
      },
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Create a black image\n",
        "image = torch.zeros(size=(3, 256, 256), dtype=torch.int)\n",
        "print(image.shape)\n",
        "# Leave the borders and make the rest of the image Green\n",
        "image[1, 18:256 - 18, 18:256 - 18] = 255\n",
        "\n",
        "# Create a mask of the same size\n",
        "mask = torch.zeros(size=(256, 256), dtype=torch.bool)\n",
        "print(mask.shape)\n",
        "# Assuming the green region in the original image is the Region of interest, change the mask to white for that area\n",
        "mask[18:256 - 18, 18:256 - 18] = 1\n",
        "\n",
        "# Create a view of the mask with the same dimensions as the original image\n",
        "mask_expanded = mask.expand_as(image)\n",
        "print(mask.shape)\n",
        "print(mask_expanded.shape)\n",
        "\n",
        "mask_np = mask_expanded.numpy().transpose(1, 2, 0) * 255\n",
        "image_np = image.numpy().transpose(1, 2, 0)\n",
        "\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "ax[0].imshow(image_np)\n",
        "ax[1].imshow(mask_np)\n",
        "plt.show()\n",
        "\n",
        "image[0, mask] += 128\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "ax[0].imshow(image_np)\n",
        "ax[1].imshow(mask_np)\n",
        "plt.show()\n",
        "\n",
        "image[mask_expanded] += 128\n",
        "image.clamp_(0, 255)\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "ax[0].imshow(image_np)\n",
        "ax[1].imshow(mask_np)\n",
        "plt.show()"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 256, 256])\n",
            "torch.Size([256, 256])\n",
            "torch.Size([256, 256])\n",
            "torch.Size([3, 256, 256])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAC7CAYAAACend6FAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nuyQ1ZLhLUkOGuyQ19P8/n7qCvZYHvgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAC7CAYAAACend6FAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAC7CAYAAACend6FAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ntlFaAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiXBx0k3ptOI",
        "colab_type": "text"
      },
      "source": [
        "In the example above, one can also find a couple of useful tricks:\n",
        "\n",
        "\n",
        "*   `clamp` method and function is a Pytorch's analogue of NumPy's `clip` function\n",
        "*   many operations on tensors have in-place form, that does not return modified data, but change values in the tensor. The in-place version of the operation has trailing underscore according to Pytorch's naming convension - in the exmaple above it is `clamp_`\n",
        "*   tensors have the same indexing as Numpy's arrays - one can use `:` seperated range, negative indexes and so on.\n",
        "\n",
        "\n",
        ".\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "# Images and their representations\n",
        "\n",
        "Now, let's discuss images, their representations and how different Python librarties work with them. \n",
        "\n",
        "Probably, the most well-known library for image loading and simple processing is [Pillow](https://pillow.readthedocs.io/en/stable/). \n",
        "\n",
        "However, many people in deep learning area stick with OpenCV for image loading and processing with some usage of another libraries when it is justified by performance/functionality. This is because OpenCV is in general much faster than the other libraries. Here you can find a couple of benchmarks: \n",
        "\n",
        "*   https://www.kaggle.com/zfturbo/benchmark-2019-speed-of-image-reading\n",
        "*   https://github.com/albumentations-team/albumentations#benchmarking-results\n",
        "\n",
        "To sum up the benchmarks above, there are two most common image formats, PNG and JPEGs. If your data is in PNG format - use OpenCV to read it. If it is in JPEG - use libturbojpeg. For image processing, use OpenCV if possible. _We will be using PIL a lot along with these._\n",
        "\n",
        "As you will read the code from others, you may find out that some of them use Pillow/something else to read data. You should know, that color image representations in OpenCV and other libraries are different - OpenCV uses \"BGR\" channel order, while others use \"RGB\" one. \n",
        "\n",
        "To change \"BRG\" <-> \"RGB\" the only thing we need to do it to change channel order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYv4sZMmpndu",
        "colab_type": "code",
        "outputId": "48f09672-2899-4eeb-ab51-939dc4ab36b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "import cv2\n",
        "\n",
        "\n",
        "bgr_image = cv2.imread('mars.jpg') \n",
        "print(bgr_image)\n",
        "print(bgr_image.shape)\n",
        "print('break')\n",
        "print(bgr_image[:,:, ::-1])\n",
        "# remember to add your own image in case you run this block, if you want to use the same image, \n",
        "# download it from: https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcRCA40ftnscVzfV8ft8e7vIzQXfXeZdtco8nknJrfCUW6INI40U\n",
        "rgb_image = bgr_image[..., ::-1]\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "ax[0].imshow(bgr_image)\n",
        "ax[1].imshow(rgb_image)\n",
        "plt.show()"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[[5 7 8]\n",
            "  [5 7 8]\n",
            "  [5 7 8]\n",
            "  ...\n",
            "  [5 7 8]\n",
            "  [5 7 8]\n",
            "  [5 7 8]]\n",
            "\n",
            " [[5 7 8]\n",
            "  [5 7 8]\n",
            "  [5 7 8]\n",
            "  ...\n",
            "  [5 7 8]\n",
            "  [5 7 8]\n",
            "  [5 7 8]]\n",
            "\n",
            " [[5 7 8]\n",
            "  [5 7 8]\n",
            "  [5 7 8]\n",
            "  ...\n",
            "  [5 7 8]\n",
            "  [5 7 8]\n",
            "  [5 7 8]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[5 7 8]\n",
            "  [5 7 8]\n",
            "  [5 7 8]\n",
            "  ...\n",
            "  [5 7 8]\n",
            "  [5 7 8]\n",
            "  [5 7 8]]\n",
            "\n",
            " [[3 5 6]\n",
            "  [4 6 7]\n",
            "  [4 6 7]\n",
            "  ...\n",
            "  [6 8 9]\n",
            "  [6 8 9]\n",
            "  [6 8 9]]\n",
            "\n",
            " [[3 5 6]\n",
            "  [4 6 7]\n",
            "  [4 6 7]\n",
            "  ...\n",
            "  [6 8 9]\n",
            "  [6 8 9]\n",
            "  [6 8 9]]]\n",
            "(450, 682, 3)\n",
            "break\n",
            "[[[8 7 5]\n",
            "  [8 7 5]\n",
            "  [8 7 5]\n",
            "  ...\n",
            "  [8 7 5]\n",
            "  [8 7 5]\n",
            "  [8 7 5]]\n",
            "\n",
            " [[8 7 5]\n",
            "  [8 7 5]\n",
            "  [8 7 5]\n",
            "  ...\n",
            "  [8 7 5]\n",
            "  [8 7 5]\n",
            "  [8 7 5]]\n",
            "\n",
            " [[8 7 5]\n",
            "  [8 7 5]\n",
            "  [8 7 5]\n",
            "  ...\n",
            "  [8 7 5]\n",
            "  [8 7 5]\n",
            "  [8 7 5]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[8 7 5]\n",
            "  [8 7 5]\n",
            "  [8 7 5]\n",
            "  ...\n",
            "  [8 7 5]\n",
            "  [8 7 5]\n",
            "  [8 7 5]]\n",
            "\n",
            " [[6 5 3]\n",
            "  [7 6 4]\n",
            "  [7 6 4]\n",
            "  ...\n",
            "  [9 8 6]\n",
            "  [9 8 6]\n",
            "  [9 8 6]]\n",
            "\n",
            " [[6 5 3]\n",
            "  [7 6 4]\n",
            "  [7 6 4]\n",
            "  ...\n",
            "  [9 8 6]\n",
            "  [9 8 6]\n",
            "  [9 8 6]]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAACHCAYAAADtJRlTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n63E0JwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoi4TeNBGHle",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Question 10:\n",
        "\n",
        "Looking at the results above it can be said that the pixel values in the blue channels would be very small compared to red channel. True/False?\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8JVdZTTw2I-",
        "colab_type": "text"
      },
      "source": [
        "**TRUE (in BGR case)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3ugFd57zNwa",
        "colab_type": "text"
      },
      "source": [
        "# Autograd\n",
        "\n",
        "Pytorch supports automatic differentiation. The module which implements this is called **AutoGrad**. It calculates the gradients and keeps track in forward and backward passes. For primitive tensors, you need to enable or disable it using the `required_grad` flag. But, for advanced tensors, it is enabled by default"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMOp4aiou6JR",
        "colab_type": "code",
        "outputId": "f11e430c-3fdd-43df-c4fc-cae017176874",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        }
      },
      "source": [
        "a = torch.rand((3, 5), requires_grad = True)\n",
        "b = torch.randint(0,21,(3, 5), dtype=torch.float32, requires_grad = True)\n",
        "print(a)\n",
        "print(b)\n",
        "print()\n",
        "result = a*b*6\n",
        "print(result)\n",
        "print(a.grad)\n",
        "# grad can be implicitly created only for scalar outputs\n",
        "# so let's calculate the sum here so that the output becomes a scalar and we can apply a backward pass\n",
        "mean_result = result.sum()\n",
        "print(mean_result)\n",
        "print()\n",
        "# calculate gradient\n",
        "mean_result.backward()\n",
        "# print gradient of a\n",
        "print(a.grad)\n",
        "print()\n",
        "print(b.grad)\n",
        "print()\n",
        "print(b.grad)"
      ],
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.9337, 0.2769, 0.6357, 0.0320, 0.8888],\n",
            "        [0.9938, 0.8330, 0.1946, 0.1655, 0.7419],\n",
            "        [0.1421, 0.3284, 0.4615, 0.2694, 0.9638]], requires_grad=True)\n",
            "tensor([[19., 12., 13.,  2., 15.],\n",
            "        [ 0.,  4.,  6., 11.,  6.],\n",
            "        [18.,  8., 17.,  3.,  2.]], requires_grad=True)\n",
            "\n",
            "tensor([[106.4450,  19.9357,  49.5863,   0.3834,  79.9943],\n",
            "        [  0.0000,  19.9932,   7.0039,  10.9199,  26.7070],\n",
            "        [ 15.3471,  15.7652,  47.0772,   4.8493,  11.5650]],\n",
            "       grad_fn=<MulBackward0>)\n",
            "None\n",
            "tensor(415.5726, grad_fn=<SumBackward0>)\n",
            "\n",
            "tensor([[114.,  72.,  78.,  12.,  90.],\n",
            "        [  0.,  24.,  36.,  66.,  36.],\n",
            "        [108.,  48., 102.,  18.,  12.]])\n",
            "\n",
            "tensor([[5.6024, 1.6613, 3.8143, 0.1917, 5.3330],\n",
            "        [5.9627, 4.9983, 1.1673, 0.9927, 4.4512],\n",
            "        [0.8526, 1.9707, 2.7692, 1.6164, 5.7825]])\n",
            "\n",
            "tensor([[5.6024, 1.6613, 3.8143, 0.1917, 5.3330],\n",
            "        [5.9627, 4.9983, 1.1673, 0.9927, 4.4512],\n",
            "        [0.8526, 1.9707, 2.7692, 1.6164, 5.7825]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ym0Amk2IGfLx",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Question 11: \n",
        "\n",
        "Why the gradient of a is all 5s above?\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PDgGq2R0k7I",
        "colab_type": "text"
      },
      "source": [
        "As we see, Pytorch automagically calculated the gradient value for us. It looks to be the correct value - we multiplied an input by 5, so the gradient of this operation equals to 5.\n",
        "\n",
        "# Disabling Autograd for tensors\n",
        "\n",
        "We don't need to compute gradients for all the variables that are involved in the pipeline. The Pytorch API provides 2 ways to disable autograd.\n",
        "\n",
        "`detach` - returns a copy of the tensor with autograd disabled. This \n",
        "\n",
        "1.   copy is built on the same memory as the original tensor, so in-place size / stride / storage changes (such as resize_ / resizeas / set / transpose) modifications are not allowed.\n",
        "2.   torch.no_grad() - It is a context manager that allows you to guard a series of operations from autograd without creating new tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqVG9fQb0cLW",
        "colab_type": "code",
        "outputId": "0f79bcdb-2a66-4a1f-d97d-f9999d53a994",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "a = torch.rand((3, 5), requires_grad=True)\n",
        "print(a)\n",
        "print()\n",
        "detached_a = a.detach()\n",
        "print(detached_a)\n",
        "print()\n",
        "detached_result = detached_a * 5\n",
        "print(detached_result)\n",
        "print()\n",
        "result = a * 10\n",
        "print(result)\n",
        "print()\n",
        "# we cannot do backward pass that is required for autograd using multideminsional output,\n",
        "# so let's calculate the sum here\n",
        "mean_result = result.sum()\n",
        "print('mean result:', mean_result)\n",
        "mean_result.backward()\n",
        "a.grad"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.0388, 0.6878, 0.8593, 0.3522, 0.3974],\n",
            "        [0.3278, 0.5274, 0.7218, 0.5582, 0.5748],\n",
            "        [0.5920, 0.2391, 0.7932, 0.9749, 0.2967]], requires_grad=True)\n",
            "\n",
            "tensor([[0.0388, 0.6878, 0.8593, 0.3522, 0.3974],\n",
            "        [0.3278, 0.5274, 0.7218, 0.5582, 0.5748],\n",
            "        [0.5920, 0.2391, 0.7932, 0.9749, 0.2967]])\n",
            "\n",
            "tensor([[0.1941, 3.4391, 4.2966, 1.7610, 1.9868],\n",
            "        [1.6391, 2.6370, 3.6091, 2.7910, 2.8742],\n",
            "        [2.9602, 1.1957, 3.9661, 4.8745, 1.4833]])\n",
            "\n",
            "tensor([[0.3881, 6.8783, 8.5932, 3.5220, 3.9736],\n",
            "        [3.2783, 5.2740, 7.2182, 5.5820, 5.7483],\n",
            "        [5.9204, 2.3915, 7.9322, 9.7491, 2.9667]], grad_fn=<MulBackward0>)\n",
            "\n",
            "mean result: tensor(79.4159, grad_fn=<SumBackward0>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[10., 10., 10., 10., 10.],\n",
              "        [10., 10., 10., 10., 10.],\n",
              "        [10., 10., 10., 10., 10.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqpch2Be02J7",
        "colab_type": "code",
        "outputId": "f8f3acfe-a0e4-45ad-cb9a-eb6b9a9238aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "a = torch.rand((3, 5), requires_grad=True)\n",
        "with torch.no_grad():\n",
        "    detached_result = a * 5\n",
        "result = a * 10\n",
        "# we cannot do backward pass that is required for autograd using multideminsional output,\n",
        "# so let's calculate the sum here\n",
        "mean_result = result.sum()\n",
        "mean_result.backward()\n",
        "a.grad"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[10., 10., 10., 10., 10.],\n",
              "        [10., 10., 10., 10., 10.],\n",
              "        [10., 10., 10., 10., 10.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjh2rYOPJUAZ",
        "colab_type": "text"
      },
      "source": [
        "# Custom Network\n",
        "\n",
        "A fully-connected ReLU network with one hidden layer and no biases, trained to predict y from x by minimizing squared Euclidean distance.\n",
        "\n",
        "This implementation uses PyTorch tensors to manually compute the forward pass, loss, and backward pass.\n",
        "\n",
        "A PyTorch Tensor is basically the same as a numpy array: it does not know anything about deep learning or computational graphs or gradients, and is just a generic n-dimensional array to be used for arbitrary numeric computation.\n",
        "\n",
        "The biggest difference between a numpy array and a PyTorch Tensor is that a PyTorch Tensor can run on either CPU or GPU. To run operations on the GPU, just cast the Tensor to a cuda datatype."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nf5RaB104Vp",
        "colab_type": "code",
        "outputId": "ab6e7a58-193b-4f5a-999e-6d9d70b2e1b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "dtype = torch.float\n",
        "device = torch.device(\"cpu\")\n",
        "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
        "\n",
        "# N is batch size; D_in is input dimension;\n",
        "# H is hidden dimension; D_out is output dimension.\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "# Create random input and output data\n",
        "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
        "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
        "\n",
        "# Randomly initialize weights\n",
        "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
        "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(500):\n",
        "    # Forward pass: compute predicted y\n",
        "    h = x.mm(w1)\n",
        "    h_relu = h.clamp(min=0)\n",
        "    y_pred = h_relu.mm(w2)\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = (y_pred - y).pow(2).sum().item()\n",
        "    print(t, loss)\n",
        "\n",
        "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
        "    grad_y_pred = 2*(y_pred - y)\n",
        "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
        "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
        "    grad_h = grad_h_relu.clone()\n",
        "    grad_h[h < 0] = 0\n",
        "    grad_w1 = x.t().mm(grad_h)\n",
        "\n",
        "    # Update weights using gradient descent\n",
        "    w1 -= learning_rate * grad_w1\n",
        "    w2 -= learning_rate * grad_w2"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 34056640.0\n",
            "1 32800776.0\n",
            "2 34710580.0\n",
            "3 33638512.0\n",
            "4 27025384.0\n",
            "5 17290406.0\n",
            "6 9334626.0\n",
            "7 4753064.5\n",
            "8 2592421.0\n",
            "9 1618996.625\n",
            "10 1153078.375\n",
            "11 898144.25\n",
            "12 736308.75\n",
            "13 620461.5\n",
            "14 530933.625\n",
            "15 458723.6875\n",
            "16 399092.71875\n",
            "17 349096.9375\n",
            "18 306786.6875\n",
            "19 270757.0625\n",
            "20 239880.75\n",
            "21 213264.0625\n",
            "22 190193.734375\n",
            "23 170152.8125\n",
            "24 152666.09375\n",
            "25 137341.4375\n",
            "26 123872.875\n",
            "27 111997.0859375\n",
            "28 101521.375\n",
            "29 92241.765625\n",
            "30 83971.4765625\n",
            "31 76582.34375\n",
            "32 69962.9921875\n",
            "33 64019.0546875\n",
            "34 58672.91796875\n",
            "35 53859.6015625\n",
            "36 49511.59375\n",
            "37 45572.31640625\n",
            "38 41999.30859375\n",
            "39 38766.3671875\n",
            "40 35829.671875\n",
            "41 33150.6328125\n",
            "42 30703.203125\n",
            "43 28464.560546875\n",
            "44 26413.447265625\n",
            "45 24531.38671875\n",
            "46 22802.08984375\n",
            "47 21211.650390625\n",
            "48 19747.48828125\n",
            "49 18397.64453125\n",
            "50 17151.88671875\n",
            "51 16001.2080078125\n",
            "52 14937.79296875\n",
            "53 13954.79296875\n",
            "54 13044.3662109375\n",
            "55 12200.1337890625\n",
            "56 11416.763671875\n",
            "57 10689.4541015625\n",
            "58 10013.9248046875\n",
            "59 9385.748046875\n",
            "60 8801.2998046875\n",
            "61 8257.03125\n",
            "62 7750.01611328125\n",
            "63 7277.275390625\n",
            "64 6836.341796875\n",
            "65 6424.57470703125\n",
            "66 6040.01953125\n",
            "67 5680.7890625\n",
            "68 5344.88720703125\n",
            "69 5030.90087890625\n",
            "70 4736.8544921875\n",
            "71 4461.53857421875\n",
            "72 4203.75439453125\n",
            "73 3962.1103515625\n",
            "74 3735.572265625\n",
            "75 3523.0947265625\n",
            "76 3323.62255859375\n",
            "77 3136.37841796875\n",
            "78 2960.557861328125\n",
            "79 2795.420166015625\n",
            "80 2640.1484375\n",
            "81 2494.17626953125\n",
            "82 2356.878173828125\n",
            "83 2227.72900390625\n",
            "84 2106.45654296875\n",
            "85 1992.2974853515625\n",
            "86 1884.829833984375\n",
            "87 1783.60888671875\n",
            "88 1688.1861572265625\n",
            "89 1598.203369140625\n",
            "90 1513.3743896484375\n",
            "91 1433.343017578125\n",
            "92 1357.814453125\n",
            "93 1286.571044921875\n",
            "94 1219.335205078125\n",
            "95 1155.8480224609375\n",
            "96 1095.8564453125\n",
            "97 1039.166015625\n",
            "98 985.5866088867188\n",
            "99 934.9861450195312\n",
            "100 887.111328125\n",
            "101 841.8292236328125\n",
            "102 799.0091552734375\n",
            "103 758.487060546875\n",
            "104 720.1343383789062\n",
            "105 683.845947265625\n",
            "106 649.531494140625\n",
            "107 617.0413208007812\n",
            "108 586.260986328125\n",
            "109 557.1046142578125\n",
            "110 529.4721069335938\n",
            "111 503.2926025390625\n",
            "112 478.4783630371094\n",
            "113 454.9522399902344\n",
            "114 432.64605712890625\n",
            "115 411.4891357421875\n",
            "116 391.42425537109375\n",
            "117 372.3757019042969\n",
            "118 354.3072509765625\n",
            "119 337.1634826660156\n",
            "120 320.8920593261719\n",
            "121 305.44244384765625\n",
            "122 290.77423095703125\n",
            "123 276.8501892089844\n",
            "124 263.62078857421875\n",
            "125 251.05645751953125\n",
            "126 239.11558532714844\n",
            "127 227.7795867919922\n",
            "128 217.00811767578125\n",
            "129 206.76962280273438\n",
            "130 197.03994750976562\n",
            "131 187.7907257080078\n",
            "132 178.9937744140625\n",
            "133 170.62660217285156\n",
            "134 162.66757202148438\n",
            "135 155.0952606201172\n",
            "136 147.8927001953125\n",
            "137 141.03817749023438\n",
            "138 134.5208282470703\n",
            "139 128.31297302246094\n",
            "140 122.40496826171875\n",
            "141 116.78207397460938\n",
            "142 111.42595672607422\n",
            "143 106.32771301269531\n",
            "144 101.4719009399414\n",
            "145 96.84744262695312\n",
            "146 92.4419174194336\n",
            "147 88.24549865722656\n",
            "148 84.24908447265625\n",
            "149 80.43864440917969\n",
            "150 76.80784606933594\n",
            "151 73.34793090820312\n",
            "152 70.0510025024414\n",
            "153 66.90660858154297\n",
            "154 63.90745544433594\n",
            "155 61.050785064697266\n",
            "156 58.32606887817383\n",
            "157 55.728328704833984\n",
            "158 53.249603271484375\n",
            "159 50.88452911376953\n",
            "160 48.628822326660156\n",
            "161 46.478179931640625\n",
            "162 44.42692565917969\n",
            "163 42.467140197753906\n",
            "164 40.59714126586914\n",
            "165 38.81380081176758\n",
            "166 37.11069107055664\n",
            "167 35.48524475097656\n",
            "168 33.9331169128418\n",
            "169 32.451576232910156\n",
            "170 31.036922454833984\n",
            "171 29.68561553955078\n",
            "172 28.39508056640625\n",
            "173 27.163448333740234\n",
            "174 25.9866886138916\n",
            "175 24.862354278564453\n",
            "176 23.788551330566406\n",
            "177 22.762184143066406\n",
            "178 21.78226089477539\n",
            "179 20.845165252685547\n",
            "180 19.950197219848633\n",
            "181 19.09490394592285\n",
            "182 18.276500701904297\n",
            "183 17.495580673217773\n",
            "184 16.747900009155273\n",
            "185 16.034181594848633\n",
            "186 15.351622581481934\n",
            "187 14.697883605957031\n",
            "188 14.073607444763184\n",
            "189 13.477136611938477\n",
            "190 12.906364440917969\n",
            "191 12.359890937805176\n",
            "192 11.83803939819336\n",
            "193 11.338415145874023\n",
            "194 10.860772132873535\n",
            "195 10.403360366821289\n",
            "196 9.966052055358887\n",
            "197 9.547796249389648\n",
            "198 9.14758014678955\n",
            "199 8.763863563537598\n",
            "200 8.397283554077148\n",
            "201 8.046247482299805\n",
            "202 7.710227012634277\n",
            "203 7.388814449310303\n",
            "204 7.080937385559082\n",
            "205 6.786227703094482\n",
            "206 6.5042338371276855\n",
            "207 6.234268665313721\n",
            "208 5.975848197937012\n",
            "209 5.728304862976074\n",
            "210 5.491381645202637\n",
            "211 5.264081001281738\n",
            "212 5.04667854309082\n",
            "213 4.838666915893555\n",
            "214 4.6390790939331055\n",
            "215 4.448025226593018\n",
            "216 4.265389442443848\n",
            "217 4.09004020690918\n",
            "218 3.9220752716064453\n",
            "219 3.761260986328125\n",
            "220 3.607046604156494\n",
            "221 3.4597859382629395\n",
            "222 3.318270206451416\n",
            "223 3.1825754642486572\n",
            "224 3.052567481994629\n",
            "225 2.928213119506836\n",
            "226 2.8089046478271484\n",
            "227 2.6945343017578125\n",
            "228 2.5848093032836914\n",
            "229 2.4797897338867188\n",
            "230 2.379206657409668\n",
            "231 2.2825024127960205\n",
            "232 2.1901354789733887\n",
            "233 2.101331949234009\n",
            "234 2.0163581371307373\n",
            "235 1.9349002838134766\n",
            "236 1.8566055297851562\n",
            "237 1.7815437316894531\n",
            "238 1.7097141742706299\n",
            "239 1.6409155130386353\n",
            "240 1.5747299194335938\n",
            "241 1.5113271474838257\n",
            "242 1.4504815340042114\n",
            "243 1.392232060432434\n",
            "244 1.3363313674926758\n",
            "245 1.2827329635620117\n",
            "246 1.2313629388809204\n",
            "247 1.1820321083068848\n",
            "248 1.13457190990448\n",
            "249 1.0892099142074585\n",
            "250 1.0456749200820923\n",
            "251 1.0038529634475708\n",
            "252 0.9637324213981628\n",
            "253 0.9253312945365906\n",
            "254 0.8884928822517395\n",
            "255 0.8528655767440796\n",
            "256 0.8189748525619507\n",
            "257 0.786360502243042\n",
            "258 0.7550552487373352\n",
            "259 0.7250768542289734\n",
            "260 0.6962325572967529\n",
            "261 0.668590247631073\n",
            "262 0.6420386433601379\n",
            "263 0.6165562272071838\n",
            "264 0.5921268463134766\n",
            "265 0.5686210989952087\n",
            "266 0.5461059212684631\n",
            "267 0.5244911313056946\n",
            "268 0.5037455558776855\n",
            "269 0.48381665349006653\n",
            "270 0.46469613909721375\n",
            "271 0.4463278353214264\n",
            "272 0.42869025468826294\n",
            "273 0.41179603338241577\n",
            "274 0.39553189277648926\n",
            "275 0.3798748254776001\n",
            "276 0.36494889855384827\n",
            "277 0.3506162166595459\n",
            "278 0.3368509113788605\n",
            "279 0.32352596521377563\n",
            "280 0.3108629584312439\n",
            "281 0.29862290620803833\n",
            "282 0.28684481978416443\n",
            "283 0.27558833360671997\n",
            "284 0.26482728123664856\n",
            "285 0.25445830821990967\n",
            "286 0.24446582794189453\n",
            "287 0.23493288457393646\n",
            "288 0.22573792934417725\n",
            "289 0.216912642121315\n",
            "290 0.20841437578201294\n",
            "291 0.20020943880081177\n",
            "292 0.19240926206111908\n",
            "293 0.18490706384181976\n",
            "294 0.17763660848140717\n",
            "295 0.17068494856357574\n",
            "296 0.1640738695859909\n",
            "297 0.15768106281757355\n",
            "298 0.15153737366199493\n",
            "299 0.1456204354763031\n",
            "300 0.13990889489650726\n",
            "301 0.1344587206840515\n",
            "302 0.12926462292671204\n",
            "303 0.12427236884832382\n",
            "304 0.11943905800580978\n",
            "305 0.11478839814662933\n",
            "306 0.11034384369850159\n",
            "307 0.10604630410671234\n",
            "308 0.10189999639987946\n",
            "309 0.09793122112751007\n",
            "310 0.09413956105709076\n",
            "311 0.09048395603895187\n",
            "312 0.08699870109558105\n",
            "313 0.08363799750804901\n",
            "314 0.08037162572145462\n",
            "315 0.07725878059864044\n",
            "316 0.07427341490983963\n",
            "317 0.07137488573789597\n",
            "318 0.06863632053136826\n",
            "319 0.06599728018045425\n",
            "320 0.06343613564968109\n",
            "321 0.06096910312771797\n",
            "322 0.058638639748096466\n",
            "323 0.056371696293354034\n",
            "324 0.05422236770391464\n",
            "325 0.05212303623557091\n",
            "326 0.050110675394535065\n",
            "327 0.04817575216293335\n",
            "328 0.046320170164108276\n",
            "329 0.04455536603927612\n",
            "330 0.04283038526773453\n",
            "331 0.04116503894329071\n",
            "332 0.03959677368402481\n",
            "333 0.03808429464697838\n",
            "334 0.036611445248126984\n",
            "335 0.035208359360694885\n",
            "336 0.033865682780742645\n",
            "337 0.03254914656281471\n",
            "338 0.03131544217467308\n",
            "339 0.030108872801065445\n",
            "340 0.02897140569984913\n",
            "341 0.027861852198839188\n",
            "342 0.026796959340572357\n",
            "343 0.02578422799706459\n",
            "344 0.024807311594486237\n",
            "345 0.02386513538658619\n",
            "346 0.022947516292333603\n",
            "347 0.0220808032900095\n",
            "348 0.021240713074803352\n",
            "349 0.020440321415662766\n",
            "350 0.019658705219626427\n",
            "351 0.018907450139522552\n",
            "352 0.018191631883382797\n",
            "353 0.01749798096716404\n",
            "354 0.01683744415640831\n",
            "355 0.016207585111260414\n",
            "356 0.015600237064063549\n",
            "357 0.015007360838353634\n",
            "358 0.014446706511080265\n",
            "359 0.013894127681851387\n",
            "360 0.013373281806707382\n",
            "361 0.012867532670497894\n",
            "362 0.012392541393637657\n",
            "363 0.011927494779229164\n",
            "364 0.01147675421088934\n",
            "365 0.011049813590943813\n",
            "366 0.010638322681188583\n",
            "367 0.010239902883768082\n",
            "368 0.009866501204669476\n",
            "369 0.009499911218881607\n",
            "370 0.00914251059293747\n",
            "371 0.008805956691503525\n",
            "372 0.008483486250042915\n",
            "373 0.00817143451422453\n",
            "374 0.007873988710343838\n",
            "375 0.007582857273519039\n",
            "376 0.0073059662245213985\n",
            "377 0.007039886899292469\n",
            "378 0.0067854770459234715\n",
            "379 0.006534979213029146\n",
            "380 0.0063020591624081135\n",
            "381 0.006078328005969524\n",
            "382 0.005858112592250109\n",
            "383 0.005643125157803297\n",
            "384 0.00544158648699522\n",
            "385 0.005248104687780142\n",
            "386 0.005057765636593103\n",
            "387 0.004882487002760172\n",
            "388 0.004708721302449703\n",
            "389 0.00454168813303113\n",
            "390 0.004382086917757988\n",
            "391 0.004227551631629467\n",
            "392 0.004076467361301184\n",
            "393 0.003933151718229055\n",
            "394 0.003797877347096801\n",
            "395 0.003660794347524643\n",
            "396 0.003536584321409464\n",
            "397 0.0034133708104491234\n",
            "398 0.003296682145446539\n",
            "399 0.0031833485700190067\n",
            "400 0.0030759740620851517\n",
            "401 0.002971499226987362\n",
            "402 0.0028711240738630295\n",
            "403 0.0027733328752219677\n",
            "404 0.0026807368267327547\n",
            "405 0.0025891736149787903\n",
            "406 0.0025038053281605244\n",
            "407 0.002420855686068535\n",
            "408 0.0023380531929433346\n",
            "409 0.002261044457554817\n",
            "410 0.002188547980040312\n",
            "411 0.002118030097335577\n",
            "412 0.002048405585810542\n",
            "413 0.0019817701540887356\n",
            "414 0.0019161866512149572\n",
            "415 0.001857925788499415\n",
            "416 0.0017990234773606062\n",
            "417 0.0017413250170648098\n",
            "418 0.0016889002872630954\n",
            "419 0.001635606400668621\n",
            "420 0.0015846792375668883\n",
            "421 0.001535662915557623\n",
            "422 0.0014883470721542835\n",
            "423 0.0014430074952542782\n",
            "424 0.0013995337067171931\n",
            "425 0.00135648506693542\n",
            "426 0.0013179361121729016\n",
            "427 0.0012783624697476625\n",
            "428 0.0012390792835503817\n",
            "429 0.0012029979843646288\n",
            "430 0.0011668408988043666\n",
            "431 0.0011319799814373255\n",
            "432 0.0010980917140841484\n",
            "433 0.0010649701580405235\n",
            "434 0.0010364692425355315\n",
            "435 0.0010057099862024188\n",
            "436 0.0009786369046196342\n",
            "437 0.0009503712644800544\n",
            "438 0.000924313033465296\n",
            "439 0.0008985530585050583\n",
            "440 0.0008750142878852785\n",
            "441 0.0008495670044794679\n",
            "442 0.0008250849205069244\n",
            "443 0.0008032762561924756\n",
            "444 0.0007819428574293852\n",
            "445 0.0007608005544170737\n",
            "446 0.0007409090176224709\n",
            "447 0.0007215941441245377\n",
            "448 0.0007029564585536718\n",
            "449 0.0006850539357401431\n",
            "450 0.0006665443070232868\n",
            "451 0.000649160414468497\n",
            "452 0.0006326232687570155\n",
            "453 0.0006175420712679625\n",
            "454 0.0006008202326484025\n",
            "455 0.0005843841354362667\n",
            "456 0.0005709201795980334\n",
            "457 0.0005562247824855149\n",
            "458 0.0005426970892585814\n",
            "459 0.000528710603248328\n",
            "460 0.0005152492085471749\n",
            "461 0.0005033439374528825\n",
            "462 0.000491199316456914\n",
            "463 0.0004782713658642024\n",
            "464 0.0004675998061429709\n",
            "465 0.00045606179628521204\n",
            "466 0.0004455256275832653\n",
            "467 0.0004353431868366897\n",
            "468 0.00042532954830676317\n",
            "469 0.0004149495798628777\n",
            "470 0.000405945407692343\n",
            "471 0.0003964293282479048\n",
            "472 0.0003876109840348363\n",
            "473 0.0003792017523664981\n",
            "474 0.00037040532333776355\n",
            "475 0.0003617069742176682\n",
            "476 0.0003538376768119633\n",
            "477 0.00034620630322024226\n",
            "478 0.0003387874457985163\n",
            "479 0.0003316778165753931\n",
            "480 0.00032431638101115823\n",
            "481 0.00031753905932419\n",
            "482 0.00031084890360943973\n",
            "483 0.00030427967431023717\n",
            "484 0.00029811650165356696\n",
            "485 0.0002914304786827415\n",
            "486 0.00028486934024840593\n",
            "487 0.00027923358720727265\n",
            "488 0.000273374083917588\n",
            "489 0.00026760163018479943\n",
            "490 0.00026268395595252514\n",
            "491 0.0002574783284217119\n",
            "492 0.00025228920276276767\n",
            "493 0.00024698040215298533\n",
            "494 0.00024208409013226628\n",
            "495 0.00023724260972812772\n",
            "496 0.00023221762967295945\n",
            "497 0.00022824134794063866\n",
            "498 0.00022425387578550726\n",
            "499 0.00022006110521033406\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycwxAPHZLNST",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "## Question 12\n",
        "\n",
        "In the code above, why do we have 2 in '2.0*(y_pred - y)`?\n",
        "\n",
        "## Question 13\n",
        "In the code above, what does `grad_h[h < 0] = 0` signify?\n",
        "\n",
        "## Question 14\n",
        "In the code above, how many \"epochs\" have we trained the model for? \n",
        "\n",
        "## Question 15\n",
        "In the code above, if we take the trained model, and run it on fresh  inputs, the trained model will be able to predict fresh output with high accuracy. \n",
        "\n",
        "## Question 16\n",
        "In the code above, if we dont use clone in `grad_h = grad_h_relu.clone()` the model will still train without any issues. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0Lo_GK4JKmh",
        "colab_type": "text"
      },
      "source": [
        "**12. After diffrentiation, power comes in front.**\n",
        "\n",
        "**13. Changing all value of less than zero to Zero, so to not have any negative numbers**\n",
        "\n",
        "**14. 500**\n",
        "\n",
        "**15. no, since it is some random data.**\n",
        "\n",
        "**16.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnJ49I07JJI6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}